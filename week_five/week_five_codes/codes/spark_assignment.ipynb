{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7c0a0b-5e98-4cdf-8865-fa7b70d709fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import types as T\n",
    "import findspark \n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579c3cf2-021e-4f8f-a41e-7a61d0c58048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 17:58:41 WARN Utils: Your hostname, idowu-pc resolves to a loopback address: 127.0.1.1; using 192.168.0.115 instead (on interface wlo1)\n",
      "23/03/05 17:58:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 17:58:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.master('local[*]').appName('spark_assignment').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2b2d7a-9a78-42d4-ab5a-a7e524a2cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca00f1-3496-41b6-b2aa-82ff2df46f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://opendata.reseaux-energies.fr/explore/dataset/eco2mix-national-tr/download/?format=csv\"\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "     \n",
    "path  = SparkFiles.get('download')\n",
    "df = spark.read.csv(\"file://\" + path, header=True, inferSchema= True, sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c216b92f-48f4-4db3-acec-ce8ce2c0fe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\"\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark_session.sparkContext.addFile(url)\n",
    "\n",
    "# path = SparkFiles.get('download')\n",
    "\n",
    "df_fhv = spark_session.read.csv(\"file://\" + SparkFiles.get(\"fhvhv_tripdata_2021-06.csv.gz\"), header=True, inferSchema=True, sep =\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98282023-80ff-41f5-9821-181332384153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 17:23:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv.repartition(12).write.mode('overwrite').parquet('data/raw/fhv/2021/06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503d467b-0388-43d0-8537-b89dfe846b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 24MB\n",
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fc08ba-2420-4fa0-a1ca-ad37ae3b1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04364ba-8b72-4d2b-8fb5-48f677edf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.createOrReplaceTempView('fhv_june_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254af011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_session.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM fhv_june_data\n",
    "WHERE CAST(pickup_datetime AS DATE) BETWEEN '2021-06-15' AND '2021-06-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b93dbe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+---------------+\n",
      "|max(timestampdiff(hour, pickup_datetime, dropoff_datetime))|pickup_datetime|\n",
      "+-----------------------------------------------------------+---------------+\n",
      "|                                                         66|     2021-06-25|\n",
      "|                                                         25|     2021-06-22|\n",
      "|                                                         19|     2021-06-27|\n",
      "|                                                         18|     2021-06-26|\n",
      "|                                                         16|     2021-06-23|\n",
      "|                                                         13|     2021-06-24|\n",
      "|                                                         11|     2021-06-04|\n",
      "|                                                         10|     2021-06-01|\n",
      "|                                                         10|     2021-06-20|\n",
      "|                                                          9|     2021-06-08|\n",
      "|                                                          9|     2021-06-19|\n",
      "|                                                          9|     2021-06-09|\n",
      "|                                                          9|     2021-06-11|\n",
      "|                                                          9|     2021-06-03|\n",
      "|                                                          9|     2021-06-30|\n",
      "|                                                          9|     2021-06-28|\n",
      "|                                                          9|     2021-06-18|\n",
      "|                                                          9|     2021-06-15|\n",
      "|                                                          8|     2021-06-17|\n",
      "|                                                          8|     2021-06-02|\n",
      "+-----------------------------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# question 4\n",
    "spark_session.sql(\"\"\"\n",
    "SELECT MAX(datediff(hour,pickup_datetime, dropoff_datetime)),CAST(pickup_datetime AS DATE)\n",
    "FROM fhv_june_data\n",
    "GROUP BY CAST(pickup_datetime AS DATE)\n",
    "ORDER BY 1 DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10d5bd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 18:36:02 WARN SparkContext: The path https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "URL_ZONE = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "spark_session.sparkContext.addFile(URL_ZONE)\n",
    "\n",
    "df_zone = spark_session.read.csv(\"file://\"+SparkFiles.get('taxi_zone_lookup.csv'), header=True, inferSchema=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca2e8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a57fe290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.createOrReplaceTempView('zone_lookup_table')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2065914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                zone|count(1)|\n",
      "+--------------------+--------+\n",
      "| Crown Heights North|  231279|\n",
      "|        East Village|  221244|\n",
      "|         JFK Airport|  188867|\n",
      "|      Bushwick South|  187929|\n",
      "|       East New York|  186780|\n",
      "|TriBeCa/Civic Center|  164344|\n",
      "|   LaGuardia Airport|  161596|\n",
      "|            Union Sq|  158937|\n",
      "|        West Village|  154698|\n",
      "|             Astoria|  152493|\n",
      "|     Lower East Side|  151020|\n",
      "|        East Chelsea|  147673|\n",
      "|Central Harlem North|  146402|\n",
      "|Williamsburg (Nor...|  143683|\n",
      "|          Park Slope|  143594|\n",
      "|  Stuyvesant Heights|  141427|\n",
      "|        Clinton East|  139611|\n",
      "|West Chelsea/Huds...|  139431|\n",
      "|             Bedford|  138428|\n",
      "|         Murray Hill|  137879|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# question 6\n",
    "spark_session.sql(\"\"\"\n",
    "WITH fhv_data AS (SELECT * \n",
    "FROM fhv_june_data),\n",
    "\n",
    "zone_data AS (SELECT * FROM\n",
    "zone_lookup_table)\n",
    "\n",
    "SELECT pick_off_zone.zone, COUNT(*)\n",
    "FROM \n",
    "fhv_data AS fd\n",
    "INNER JOIN zone_lookup_table as pick_off_zone\n",
    "ON fd.PULocationID = pick_off_zone.LocationID\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
